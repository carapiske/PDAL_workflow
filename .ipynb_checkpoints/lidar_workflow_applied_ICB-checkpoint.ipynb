{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797f557e-435f-49cd-8002-2135758b8c67",
   "metadata": {},
   "source": [
    "# Lidar Processing Workflow\n",
    "**Cara Piske, Graduate Program of Hydrologic Sciences, 2022; Advisor: Dr. Adrian Harpold**<br>\n",
    "This code processes raw lidar point clouds in order to calculate snow depth using PDAL. <br>\n",
    "Lidar data were provided by the Airborne Snow Observatory (ASO), the National Center for Airborne Laser Mapping (NCALM), and Watershed Sciences Inc. (WSI). <br>\n",
    "\n",
    "The goal of this project is to process snow depth to the one-meter spatial scale while maintaining conservative under-canopy estimates. Therefore, little interpolation occurs under-canopy. We follow these protocols in order to obtain a 1-m rasterized product (as opposed to the 3-m rasterized product provided by ASO on the NSIDC data portal). NCALM and WSI flights were obtained through OpenTopography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7346b-ed91-48fe-912a-4dc9d7b20be4",
   "metadata": {},
   "source": [
    "Start by importing necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef19ecac-1150-469a-9a3f-3eb6f8b1c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary files\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import json # where we will save the json files to run a pipeline\n",
    "import os # for file management\n",
    "import subprocess # allows us to run command line commands\n",
    "import pdal # lidar processing package\n",
    "import rasterio\n",
    "import pyproj\n",
    "from osgeo import gdal\n",
    "\n",
    "import time\n",
    "# packages to copy files\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "# packages to extract wkt from polygon\n",
    "import shapefile\n",
    "import pygeoif\n",
    "# for parallel processing\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "#import seaborn as sns\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import kde\n",
    "from scipy.stats import gaussian_kde\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# See lidar_functions.py\n",
    "import lidar_functions\n",
    "pdal_pipeline = 'C:\\\\Users\\cpiske\\.conda\\envs\\lidar\\Lib\\site-packages\\pdal\\pipeline.py'\n",
    "gdal_merge = os.path.join('C:\\\\','Users','cpiske','.conda','envs','lidar','Lib','site-packages','osgeo_utils','gdal_merge.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec494a-5f52-4fc5-8ced-a5886f938892",
   "metadata": {},
   "source": [
    "Note that many functions are dependent on specific directory structures. See README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eb2762-4c68-4a8e-a2e0-3c4434606bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_base_path = 'piske_processing/PDAL_workflow/JSON/' # set so that we can redefine json across operating systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933dbf5-603d-4eac-a4df-8ec90e6278d4",
   "metadata": {},
   "source": [
    "Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a6c326-633d-4550-b011-0f012e2b2825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set current working directory\n",
    "path = '/'\n",
    "os.chdir(path)\n",
    "os.getcwd() # print to ensure we're in the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f30faa-fe09-4f19-b736-214b2aad11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_base_path = 'piske_processing/PDAL_workflow/JSON/' # set so that we can redefine json across operating systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "600a92ec-7986-41ff-99d1-28daea0535a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all json files\n",
    "filterMergeRasterize_json = json_base_path + 'filterMergeRasterize.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9edcfd-2013-4858-90a5-d9ca09c35507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All json paths\n",
    "DEM_ind_json = json_base_path+'DEM_from_individual_las.json' \n",
    "DEM_json = json_base_path+'DEM_from_las.json' \n",
    "HAG_json = json_base_path + 'HAG_dem.json'\n",
    "clip_json = json_base_path +'clip_to_polygon.json'\n",
    "extract_las_atPoint = json_base_path +'extract_las_atPoint.json'\n",
    "extract_las_atPolygon = json_base_path +'extract_las_atPolygon.json'\n",
    "correct_merge_rasterize_json = json_base_path + 'correct_merge_rasterize.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371e470-c770-4b4d-93a4-52d882661c02",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019afb1c-f0f0-4e99-8072-cdabfaee5be6",
   "metadata": {},
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff1251-3a50-4fec-9e91-4837b39954f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input file\n",
    "input_lid = r'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/DEM/WSI_NCALM_BE.tif'\n",
    "pdal_info_cmd = ['pdal','info',input_lid] # general info\n",
    "pdal_metadata_cmd = ['pdal','info',input_lid,'--metadata'] # full file metadata, including details crs\n",
    "subprocess.run(pdal_metadata_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecd2f6-067f-4c0a-bba7-86442f1d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal_info_results = subprocess.run(pdal_metadata_cmd, stdout = subprocess.PIPE) # stout (standard out), PIPE indicates that a new pipe to the child should be created\n",
    "pdal_info_dict = json.loads(pdal_info_results.stdout.decode()) # create dict with metadata info\n",
    "#pdal_info_dict # print results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7cd5d-0ac8-419c-9d36-fa0589d65e04",
   "metadata": {},
   "source": [
    "Check extents (in order to create bounds for rasterization below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa978a-0e82-4d69-81f3-fcc1a59e258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_folder = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/NAD83_NAD83_epoch2010/'\n",
    "# lidar_folder = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_2014/NAD83_NAD83_epoch2010/'\n",
    "onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "full_paths = [lidar_folder +f for f in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0990454-2c2d-4b1b-906a-0561933b7954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain coordinates of file to use to set bounding box of merged files\n",
    "raster_filename = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/DEM/WSI_NCALM_BE.tif'\n",
    "src = gdal.Open(raster_filename) # open source file\n",
    "ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform() \n",
    "lrx = ulx + (src.RasterXSize * xres) # lower right x\n",
    "lry = uly + (src.RasterYSize * yres) # lower right y\n",
    "src = None # close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516d352-8076-4ecd-89bd-2ca235b8ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_bounds = ([str(ulx),str(lrx)],[str(lry),str(uly)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb94812-360f-4f0d-9d24-cf8f351303ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use writers_bounds from the combined tindex\n",
    "writers_bounds = '([268000,286512.9],[4165234.9,4179857.7])'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956de29-931e-492c-b2d9-219f92684d9f",
   "metadata": {},
   "source": [
    "### Counts\n",
    "Goal: filter pixels with low point densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc05e6d-c177-48c3-b092-1154f8827ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_folder = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/NAD83_NAD83_epoch2010/'\n",
    "# output_folder = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/counts/'\n",
    "\n",
    "# tic = time.perf_counter()\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "#         onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "#         full_path = [os.path.join(lidar_folder, s) for s in onlyfiles]\n",
    "#         output_path = [output_folder + s[:-3] + 'tif' for s in onlyfiles]\n",
    "#         executor.map(lidar_functions.rasterize_count, full_path, output_path) \n",
    "# toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ad045-40a9-4f2b-9fc6-18395a684b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f00b2aa-a8ad-43da-99ef-cc203c9bd7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_json = json_base_path+'count_from_las.json' # define path to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "021c10fb-2dc2-4e80-a9fa-2ec7635f87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "output_tif = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/counts/NCALM_MRB_20180921_counts_gt2m.tif'\n",
    "onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "input_list = [input_path + s for s in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f0fbdeb6-08fe-4691-9e77-56d4ec44e56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc4a4073-ee09-47e7-8f81-8b97c2702818",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dict = {}\n",
    "tags = ['']*len(input_list)\n",
    "filenames = ['']*len(input_list)\n",
    "for i in range(len(input_list)):\n",
    "    filename_dict['filename_'+str(i)] = {'filename':input_list[i], 'tag':'A_'+str(i)}\n",
    "    tags[i] = 'A_'+str(i)\n",
    "    filenames[i] = filename_dict[list(filename_dict)[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf5cf48d-0840-40fc-a290-8e91b800f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge all las files or stages\n",
    "filter_merge = {\"type\":\"filters.merge\",\n",
    "               \"tag\": \"merged\",\n",
    "               \"inputs\": tags}\n",
    "filter_range = {\"type\":\"filters.range\",\n",
    "                \"limits\": \"Z(2:100]\",\n",
    "               \"tag\": \"ranged\",\n",
    "               \"inputs\": 'merged'}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'count',\n",
    "              'resolution': '1.0',\n",
    "               'bounds': writers_bounds,\n",
    "              'radius': '0.7',\n",
    "              'filename': output_tif}\n",
    "# Append each stage to a list prior to saving to json \n",
    "# pipeline_list = input_list[0]\n",
    "pipeline_list = filenames.copy()\n",
    "pipeline_list.append(filter_merge)\n",
    "pipeline_list.append(filter_range)\n",
    "pipeline_list.append(writers_gdal)\n",
    "# pipeline_list = [input_list[0], filter_range, writers_gdal]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(count_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6311ea4b-b5d1-4ef2-9a82-14f528e9fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ea44b423-3e22-4275-897d-5681da1ea6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pdal', 'pipeline', 'piske_processing/PDAL_workflow/JSON/count_from_las.json'], returncode=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdal_cmd = ['pdal','pipeline', count_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d5f7001a-c65b-4780-a519-b84877524007",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eab88d58-5e2d-4782-a2bc-351e9d16b176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.32623959500003"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(toc-tic)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751d94d-d53d-40ce-a8d4-d62adad47b08",
   "metadata": {},
   "source": [
    "## Retile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4032e7-3646-4f0e-9d5e-acc98c7ea589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_folder = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_2014/NAD83_NAD83_epoch2010/'\n",
    "# retile_folder = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_2014/retile_uo/'\n",
    "# onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "# for files in onlyfiles:\n",
    "#     full_path = os.path.join(lidar_folder, files)\n",
    "#     output_path = retile_folder+'#' + files\n",
    "#     retile_command = ['pdal', 'tile', full_path, output_path, '--length=1000','--buffer=50']\n",
    "#     subprocess.run(retile_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26292a6b-d7e1-4671-8a2c-81b27432469c",
   "metadata": {},
   "source": [
    "## Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd725789-60cb-4aa6-b79f-dfca7e1ff3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_folder = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/NAD83_NAD83_epoch10/'\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "#         onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "#         full_path = [os.path.join(lidar_folder, s) for s in onlyfiles]\n",
    "#         executor.map(lidar_functions.rename_llx_lly_b, full_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ce35c-70dc-498a-8a67-8139c0ec8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_folder = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/NAD83_NAD83_epoch10/'\n",
    "# tic = time.perf_counter()\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n",
    "#         onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "#         full_path = [os.path.join(lidar_folder, s) for s in onlyfiles]\n",
    "#         executor.map(lidar_functions.add_str_to_filename, full_path) \n",
    "# toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97d2d1-e28e-4bed-a4ba-b0363d4434a4",
   "metadata": {},
   "source": [
    "## Save Tile Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbace8bd-7da0-4913-9960-c65f1eca84cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one folder\n",
    "# tic = time.perf_counter()\n",
    "# lidar_folder = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/laz/'\n",
    "# output_folder = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/tindex/original/'\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "#         onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "#         full_path = [os.path.join(lidar_folder, s) for s in onlyfiles]\n",
    "#         # change depending on directory formats\n",
    "#         #output_path = [os.path.join(os.path.basename(lidar_folder),'tindex/tiles/' + s[:-3] + 'sqlite') for s in onlyfiles]\n",
    "#         output_path = [output_folder + s[:-3] + 'sqlite' for s in onlyfiles]\n",
    "#         executor.map(lidar_functions.create_tindex, full_path, output_path) #running 10 times\n",
    "# toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59151d2-faa6-445f-b84a-86ecb4fa60fe",
   "metadata": {},
   "source": [
    "## Copy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528ffe9-6116-491e-967c-7fbfb1fdbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tic = time.perf_counter()\n",
    "# lidar_folder = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/laz/'\n",
    "# output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/ICB_tiles/'\n",
    "# onlyfiles = [f for f in os.listdir(lidar_folder) if os.path.isfile(os.path.join(lidar_folder, f))]\n",
    "# full_paths = [os.path.join(lidar_folder, s) for s in onlyfiles] \n",
    "# output_paths = np.repeat(output_path, len(full_paths))\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "#         executor.map(lidar_functions.copy_lid_by_ext_ICB, full_paths, output_paths)\n",
    "# toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8934cc-5009-45a1-9b5b-852df2c3a908",
   "metadata": {},
   "source": [
    "# Create DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c05e3051-45c4-4ab1-8076-68fe36d1e098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEM_json = json_base_path+'DEM_from_las.json' # define path to json files\n",
    "#DEM_json = json_base_path+'DEM_from_individual_las.json' # define path to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd7b5912-904f-4064-9a78-32ac0ad932aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/NAD83_NAD83_epoch2010/'\n",
    "output_tif = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/DEM/NCALM_MRB_20180921_BE.tif'\n",
    "onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "input_list = [input_path + s for s in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0850ec42-97c2-4944-88b4-256f8d4dbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dict = {}\n",
    "tags = ['']*len(input_list)\n",
    "filenames = ['']*len(input_list)\n",
    "for i in range(len(input_list)):\n",
    "    filename_dict['filename_'+str(i)] = {'filename':input_list[i], 'tag':'A_'+str(i)}\n",
    "    tags[i] = 'A_'+str(i)\n",
    "    filenames[i] = filename_dict[list(filename_dict)[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b114ffa3-9aef-4a9c-91cb-0528a5a27d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge all las files or stages\n",
    "filter_merge = {\"type\":\"filters.merge\",\n",
    "               \"tag\": \"merged\",\n",
    "               \"inputs\": tags}\n",
    "# filter out the ground points of the tiles\n",
    "filter_range = {\"type\":\"filters.range\",\n",
    "               \"inputs\":'merged',\n",
    "               'tag':'ranged',\n",
    "                \"limits\":\"Classification[2:2]\"}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'mean',\n",
    "              'resolution': '1.0',\n",
    "              'radius': '0.7',\n",
    "               'bounds': writers_bounds,\n",
    "               'window_size':3,\n",
    "              'filename': output_tif}\n",
    "# Append each stage to a list prior to saving to json \n",
    "# pipeline_list = input_list[0]\n",
    "pipeline_list = filenames.copy()\n",
    "pipeline_list.append(filter_merge)\n",
    "pipeline_list.append(filter_range)\n",
    "pipeline_list.append(writers_gdal)\n",
    "#pipeline_list = [input_list[0], filter_range, writers_gdal]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(DEM_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a3e9b4a-57d9-493e-8b30-8ed013f34e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c1324e2-3971-44fc-a2f8-10f60ab76ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pdal', 'pipeline', 'piske_processing/PDAL_workflow/JSON/DEM_from_las.json'], returncode=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdal_cmd = ['pdal','pipeline', DEM_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf5850c7-98d0-42b7-b1ea-9dd5b69d54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d3bf358-87f5-4874-b71b-d998270db902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.435915763333227"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(toc-tic)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d6969-cca6-4134-bfaf-c9c09b9fe17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eeb898-7c9f-4b69-b3f8-bec6da64569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEM_ind_json = json_base_path+'DEM_from_individual_las.json' # define path to json files\n",
    "# filter out the ground points of the tiles\n",
    "reader_las = {'type':'readers.las'}\n",
    "filter_range = {\"type\":\"filters.range\",\n",
    "                \"limits\":\"Classification[2:2]\"}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'mean',\n",
    "              'resolution': '1.0',\n",
    "              'radius': '0.7',\n",
    "               'window_size':'3'}\n",
    "# Append each stage to a list prior to saving to json \n",
    "pipeline_list = [filter_range, writers_gdal]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(DEM_ind_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856c600-caed-453e-9f1f-79bc48391e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/laz2/'\n",
    "output_path = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/DEM/las/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.ground_filter_rasterize, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd614be9-2539-47cc-9bc8-64bdd76a75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_merge = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/DEM/WSI_MRB_20111017_BE.tif'\n",
    "pathname = 'MRB/Merced_lidar/Watershed_Sciences/WSI_MRB_20111017/DEM/las/'\n",
    "merge_command = [\"python\", gdal_merge, \"-o\", output_merge]\n",
    "\n",
    "for path in os.listdir(pathname):\n",
    "    full_path = os.path.join(pathname, path)\n",
    "    if os.path.isfile(full_path):\n",
    "        merge_command.append(full_path)\n",
    "subprocess.call(merge_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13399307-5a71-472d-9039-262c703e83e7",
   "metadata": {},
   "source": [
    "## Heigh Above Ground\n",
    "Because the WSI data is acting strangely (i.e. I believe it is too much memory for the merge function to handle), we'll start off with the NCALM flight as the base DEM for vertical bias corrections. <br> \n",
    "We need to correct the two snow-off flights against each other in order to combine for a comprehensive DEM (the 2018 flight has less manual correction and similar point density but the WSI flight was taken pre-fire when there was a greater chance for vegetation interception - so we'll prioritize the NCALM flight when merging) <br>\n",
    "This means we need to do a bit of a background VBC and create each element of a vbc for a subset of the WSI data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf501c4d-8952-462c-9c9a-138e0b7e52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define json path \n",
    "HAG_json = json_base_path + 'HAG_dem.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac7679-13aa-430e-ad30-0c087349bb16",
   "metadata": {},
   "source": [
    "Start with the NCALM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "447a87a8-f25b-4ff8-93ab-a92c7b4bc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define json path \n",
    "HAG_json = json_base_path + 'HAG_dem.json'\n",
    "# convert all z values to the height above ground \n",
    "target_dem = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/DEM/NCALM_MRB_20180921_BE.tif'\n",
    "filter_hag = {\"type\":\"filters.hag_dem\",\n",
    "              \"raster\":target_dem, # full file path of target DEM (.tif)\n",
    "              \"zero_ground\":\"false\"} # Do not assign 0 to ground classified points\n",
    "filter_ferry = {\"type\":\"filters.ferry\",\n",
    "                \"dimensions\":\"HeightAboveGround=>Z\"} # replace all Z dimensions with HAG instead of elevation\n",
    "# filter_range = {\"type\":\"filters.range\",\n",
    "#                 \"limits\":\"Z[:1.5]\"} # apply a noise filter\n",
    "pipeline_list = [filter_hag, filter_ferry]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "with open(HAG_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8c1511e-856a-426e-9794-75b704b146bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path ='MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/NAD83_NAD83_epoch2010/'\n",
    "output_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path  + s for s in onlyfiles]\n",
    "        executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01152215-c838-40fd-b575-ee32d775fcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193f260-2b5b-424b-9ced-0eb9257ee925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f72a44-1d5f-4494-933d-ff6d9891e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/NAD83_NAD83_epoch2010/'\n",
    "output_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path  + s for s in onlyfiles]\n",
    "        executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cf81a-7f24-45a0-9d90-9bf25f80ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc= time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3618d9-4176-4ce1-95c2-f8278cc17e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a776f0e-cf76-4a23-b113-e59671f17157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/NAD83_NAD83_epoch2010/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path  + s for s in onlyfiles]\n",
    "        executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c5004-19e2-4a57-be63-7552c39d575c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1703bd-87ac-480d-8c9f-edcfd79e361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parallel processing\n",
    "# # time = 1.1 min\n",
    "# tic = time.perf_counter()\n",
    "# input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20190418_20190419/NAD83_NAD83_epoch2010/'\n",
    "# output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20190418_20190419/HAG/'\n",
    "# if __name__ == '__main__':\n",
    "#     with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "#         onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "#         full_path = [input_path + s for s in onlyfiles]\n",
    "#         output_path_full = [output_path  + s for s in onlyfiles]\n",
    "#         executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9822c-9db8-4132-b641-06605deb1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20200413_20200414/NAD83_NAD3_epoch2010/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20200413_20200414/HAG/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path  + s for s in onlyfiles]\n",
    "        executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be62742-e0d1-4f5b-b0ff-0632b0286250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/NAD83_NAD83_epoch10/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/HAG/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path  + s for s in onlyfiles]\n",
    "        executor.map(lidar_functions.HAG_dem, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f902c6-3197-4366-90f7-81961d0c3555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d62739-a7e5-49e6-8187-edadc9e3d8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad2a53a-72cd-4fe7-8c9f-fe68102cdbc5",
   "metadata": {},
   "source": [
    "# Vertical Bias Correction\n",
    "We have two snow-off flights and all target values are points in this case.<br>\n",
    "We'll start with points that we know should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bee4b-e630-4b30-9bb1-93c97baff801",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29033e0d-5e62-414e-a016-a3f4af5d4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/CHM/' # define path of input files\n",
    "# #output_fname = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/CHM/NCALM_MRB_20180921_CHM.tif'# set output filename\n",
    "# #input_fname = glob.glob(input_path) # save to list\n",
    "# onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "# full_path = [input_path + s for s in onlyfiles]\n",
    "# input_fname = full_path.copy()\n",
    "# pdal_merge_command = input_fname\n",
    "# pdal_merge_command.insert(len(pdal_merge_command),output_fname) # insert output file to list\n",
    "# pdal_merge_command.insert(0,'--files')\n",
    "# pdal_merge_command.insert(0,'merge')\n",
    "# pdal_merge_command.insert(0,'pdal')\n",
    "# subprocess.run(pdal_merge_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33edf1af-7bd4-46fd-916f-82ad5b7522bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path.insert(len(pdal_merge_command),output_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64eeb4d-1c69-44b6-b993-aa79a6db81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_json = json_base_path + 'merge_list.json'# merge all las files or stages\n",
    "# filter_merge = {\"type\":\"filters.merge\",\n",
    "#                \"tag\": \"merged\",\n",
    "#                \"inputs\": tags}\n",
    "# # Append each stage to a list prior to saving to json \n",
    "# pipeline_list = filenames.copy()\n",
    "# pipeline_list.append(filter_merge)\n",
    "# pipeline_list.append(filter_range)\n",
    "# pipeline_list.append(writers_gdal)\n",
    "#pipeline_list = [filter_range, writers_gdal]\n",
    "pipeline_dict = {'pipeline' : full_path}\n",
    "# save to json\n",
    "with open(merge_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab4a0f-6b84-48cf-a085-735757f796dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal_cmd = ['pdal','pipeline', merge_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca820b1-7dc0-4954-b1db-3ebec518e4c0",
   "metadata": {},
   "source": [
    "## Extract lidar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7551b5d-a8a1-40eb-84f5-25e940002bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name JSON file\n",
    "extract_las_atPoint_json = json_base_path +'extract_las_atPoint.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56467508-027c-4fda-b448-bcbed112e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in ground_truthed data\n",
    "def csv_to_list(input_csv):\n",
    "    src = open(input_csv)\n",
    "    csvreader = csv.reader(src)\n",
    "    header = next(csvreader)\n",
    "    output_list = []\n",
    "    for row in csvreader:\n",
    "        output_list.append(row)\n",
    "    src.close()\n",
    "    return(header, output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4742ce-3bb6-45ad-ae59-855fa60c362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/vertical_bias_correction/'\n",
    "filenames, tags = lidar_functions.create_command_template(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee3fb6-292f-4727-9bb2-f691722e7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_las_atPoint(filenames, tags, target_point, output_txt,distance):\n",
    "    # merge all las files or stages\n",
    "    filter_merge = {\"type\":\"filters.merge\",\n",
    "                    \"tag\": \"merged\",\n",
    "                    \"inputs\": tags}\n",
    "    # crop\n",
    "    filter_crop = {'type':'filters.crop',\n",
    "                   'point':target_point,\n",
    "                   'distance':distance,\n",
    "                   'inputs':'merged',\n",
    "                   'tag': 'cropped'}\n",
    "    # crop\n",
    "    filter_range = {'type':'filters.range',\n",
    "                   'limits':'Classification[2:2]',\n",
    "                   'inputs':'cropped',\n",
    "                   'tag': 'range'}\n",
    "    \n",
    "    # write merged las to raster\n",
    "    writers_gdal = {\"type\":\"writers.text\",\n",
    "                    \"format\":\"csv\",\n",
    "                    \"order\":\"Z\",\n",
    "                    \"write_header\":False,\n",
    "                    'keep_unspecified':False,\n",
    "                    'filename':output_txt}\n",
    "    # Append each stage to a list prior to saving to json \n",
    "    pipeline_list = filenames.copy()\n",
    "    pipeline_list.append(filter_merge)\n",
    "    pipeline_list.append(filter_crop)\n",
    "    pipeline_list.append(filter_range)\n",
    "    pipeline_list.append(writers_gdal)\n",
    "\n",
    "    pipeline_dict = {'pipeline' : pipeline_list}\n",
    "    # save to json\n",
    "    with open(extract_las_atPoint_json, 'w') as out:\n",
    "        json.dump(pipeline_dict, out, indent=4)\n",
    "    pdal_cmd = ['pdal','pipeline', extract_las_atPoint_json]\n",
    "    subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0026e0-8c6f-44fe-b339-aa695bba261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from field data\n",
    "SNOTEL_src = \"SCB/supporting_files/Harpold_data_paper/snowdepth_filtered.csv\"\n",
    "snotel_header, snotel_data = csv_to_list(SNOTEL_src)\n",
    "hunt_src = 'SCB/supporting_files/Huntingon_2008_snow/Hungington_2008_snow.csv'\n",
    "hunt_header, hunt_data = csv_to_list(hunt_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb5d7d-a0d8-4c35-bb52-b7723ad71e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hunt_data)):\n",
    "    for j in range(1,6):\n",
    "        output_txt = output_path+'huntington_'+str(i)+'_dist'+str(j)+'.csv'\n",
    "        target_point = 'POINT('+hunt_data[i][7]+' '+hunt_data[i][8]+')'\n",
    "        extract_las_atPoint(filenames, tags, target_point, output_txt,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dde43b-606f-4d94-a50b-fc7135ae6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = 4 min\n",
    "for i in range(len(snotel_data)):\n",
    "    for j in range(1,6):\n",
    "        output_txt = output_path+'snotel_'+str(i)+'_dist'+str(j)+'.csv'\n",
    "        target_point = 'POINT('+snotel_data[i][5]+' '+snotel_data[i][6]+')'\n",
    "        extract_las_atPoint(filenames, tags, target_point, output_txt,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc45e7-d895-43f8-9aab-af78f1328c3a",
   "metadata": {},
   "source": [
    "## Calculate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b75ae5-8299-468b-a120-67249d868a86",
   "metadata": {},
   "source": [
    "Applied\n",
    "Using 2014 NCALM Flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0cd43-447f-40c8-9bd1-73711bfbe4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASO_20160326_hwy89_stats = lidar_functions.calculate_vertical_bias('SCB/Sagehen_lidar/ASO/ASO_SCB_20160326/hwy89_vertical_bias/clipped/ASO_SCB_20160326_hwy89_clip.laz')\n",
    "ASO_20160417_hwy89_stats = lidar_functions.calculate_vertical_bias('SCB/Sagehen_lidar/ASO/ASO_SCB_20160417/hwy89_vertical_bias/clipped/ASO_SCB_20160417_hwy89_clip.laz')\n",
    "ASO_20160518_hwy89_stats = lidar_functions.calculate_vertical_bias('SCB/Sagehen_lidar/ASO/ASO_SCB_20160518/hwy89_vertical_bias/clipped/ASO_SCB_20160518_hwy89_clip.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724eab8-e366-4c29-a3d0-33d69b753df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2014_hwy89_stats = lidar_functions.calculate_vertical_bias('SCB/Sagehen_lidar/NCALM/NCALM_SCB_2014/hwy89_vertical_bias/clipped/NCALM_SCB_2014_hwy89_clip.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670984d-73d8-462e-b036-31e6d3dd2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ASO_20160326_hwy89_stats)\n",
    "print(ASO_20160417_hwy89_stats)\n",
    "print(ASO_20160518_hwy89_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456a359-3004-44c3-b3a5-e2cea8e8d92f",
   "metadata": {},
   "source": [
    "**NCALM 2008**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31fa74-0489-4f75-9ddc-2801b3bfa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_las_txt = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_20080210/SNOTEL_vertical_bias/clipped/snotel_541.csv'\n",
    "input_las_txt = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_20080210/SNOTEL_vertical_bias/clipped/snotel_539.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fe203-316a-4976-81e5-6d41d5e8730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hag_arr = np.loadtxt(input_las_txt,skiprows=1)\n",
    "lowest_10th_per = np.nanpercentile(hag_arr, 10)\n",
    "mean_hag = np.nanmean(hag_arr)\n",
    "median_hag = np.nanmedian(hag_arr)\n",
    "stats_539 = [\"lowest_10th\",lowest_10th_per, \"mean\",mean_hag, \"median\", median_hag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b2900-2d7f-41d6-a065-3cdaeb8e8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2008_vbc_dict = {}\n",
    "input_path = 'SCB/Sagehen_lidar/NCALM/NCALM_SCB_20080210/vertical_bias_correction/ground_pts/'\n",
    "onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))] # make a list of all filenames in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21b891-2b16-4b8b-b0a5-209888375c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2008_vbc_dict = {}\n",
    "all_files = glob.glob('SCB/Sagehen_lidar/NCALM/NCALM_SCB_20080210/vertical_bias_correction/ground_pts/*')\n",
    "for files in onlyfiles:\n",
    "    txt_path = input_path + files\n",
    "    txt_arr = np.loadtxt(txt_path)\n",
    "    NCALM_2008_vbc_dict[files[:-4]] = txt_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1b4d9-54b2-4c4c-9a71-8fc47f317a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2008_error_dict = {}\n",
    "keys_list = list(NCALM_2008_vbc_dict.keys())\n",
    "for i in range(len(keys_list)):\n",
    "    index = int(re.findall(r'\\d+',  keys_list[i])[0])\n",
    "    if keys_list[i][0:3]=='hun':\n",
    "        NCALM_2008_error_dict[keys_list[i]] = float(hunt_data[index][4]) - NCALM_2008_vbc_dict[keys_list[i]]\n",
    "    elif keys_list[i][0:3]=='sno':\n",
    "        NCALM_2008_error_dict[keys_list[i]] = float(snotel_data[index][4])/100 - NCALM_2008_vbc_dict[keys_list[i]]\n",
    "    else:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dc373-2439-4cb9-98ac-0d1532af3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2008_error_plot, axs = plt.subplots(5, 11, figsize =(30,10));\n",
    "#NCALM_2008_error_plot.suptitle('NCALM 2008 Lidar Depth Distributions Around Ground Truth') # add title to entire figure\n",
    "\n",
    "k = 0\n",
    "for i in range(11):\n",
    "    for j in range(5):\n",
    "        keys_list = list(NCALM_2008_vbc_dict.keys())\n",
    "        index = int(re.findall(r'\\d+',  keys_list[k])[0])\n",
    "        true_val = float(hunt_data[index][4])\n",
    "        g = sns.histplot(ax = axs[j,i], data = NCALM_2008_vbc_dict[keys_list[k]])\n",
    "        med = np.median(NCALM_2008_vbc_dict[keys_list[k]])\n",
    "        g.axvline(true_val, color='red')\n",
    "        g.axvline(med, color = 'green')\n",
    "        y_lab = str(j+1)+'m'\n",
    "        x_lab = \"Point \" +str(i)\n",
    "        axs[j,i].set(yticklabels = [], title=' ',xlabel = '',ylabel ='')\n",
    "        axs[0,0].set_ylabel('Radius = 1m'); axs[1,0].set_ylabel('Radius = 2m');axs[2,0].set_ylabel('Radius = 3m');axs[3,0].set_ylabel('Radius = 4m');axs[4,0].set_ylabel('Radius = 5m')\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b62a20-03a4-4e5a-be51-1e36ea255b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCALM_2008_error_plot, axs = plt.subplots(5, 11, figsize =(30,10));\n",
    "#NCALM_2008_error_plot.suptitle('NCALM 2008 Lidar Depth Distributions Around Ground Truth') # add title to entire figure\n",
    "\n",
    "k = 0\n",
    "for i in range(11):\n",
    "    for j in range(5):\n",
    "        keys_list = list(NCALM_2008_error_dict.keys())\n",
    "        index = int(re.findall(r'\\d+',  keys_list[k])[0])\n",
    "        #true_val = float(hunt_data[index][4])\n",
    "        g = sns.histplot(ax = axs[j,i], data = NCALM_2008_error_dict[keys_list[k]])\n",
    "        med = np.median(NCALM_2008_error_dict[keys_list[k]])\n",
    "        # g.axvline(true_val, color='red')\n",
    "        g.axvline(med, color = 'green')\n",
    "        y_lab = str(j+1)+'m'\n",
    "        x_lab = \"Point \" +str(i)\n",
    "        axs[j,i].set(yticklabels = [], title=' ',xlabel = '',ylabel ='')\n",
    "        axs[0,0].set_ylabel('Radius = 1m'); axs[1,0].set_ylabel('Radius = 2m');axs[2,0].set_ylabel('Radius = 3m');axs[3,0].set_ylabel('Radius = 4m');axs[4,0].set_ylabel('Radius = 5m')\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24815514-66ef-4dc9-9fc3-0e2506db0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(dist):\n",
    "    error_med = []\n",
    "    keys_list = list(NCALM_2008_error_dict.keys())\n",
    "    for k in range(int(len(NCALM_2008_error_dict))):\n",
    "        if int(re.findall(r'\\d+',  keys_list[k])[1])==dist:\n",
    "            error_med.extend(NCALM_2008_error_dict[keys_list[k]])\n",
    "    return(error_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9cadc-b444-4478-850f-216c1bbadc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_med_2 = calculate_error(2)\n",
    "error_med_3 = calculate_error(3)\n",
    "error_med_4 = calculate_error(4)\n",
    "error_med_5 = calculate_error(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5acebaa-af2b-4fb8-a063-2407bf1c3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Radius = 2m, median error = \",np.median(error_med_2),'stdev = ',np.std(error_med_2))\n",
    "print(\"Radius = 3m, median error = \",np.median(error_med_3),'stdev = ',np.std(error_med_3))\n",
    "print(\"Radius = 4m, median error = \",np.median(error_med_4),'stdev = ',np.std(error_med_4))\n",
    "print(\"Radius = 5m, median error = \",np.median(error_med_5),'stdev = ',np.std(error_med_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d404e-911a-431b-82b4-a3c3c84d70b4",
   "metadata": {},
   "source": [
    "hold off on corrections for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5bcbb-8274-4847-b129-d1c92bb3cb58",
   "metadata": {},
   "source": [
    "## Correct and Rasterize\n",
    "In this case we know that the May flight has the most limited extent. One way to check for this would be to use a similar code to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dcefd-7390-4041-844c-da5c58049da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_merge_rasterize_json = json_base_path + 'correct_merge_rasterize.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c9439-5269-4964-94e3-40a61bb02147",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/'\n",
    "filenames, tags = lidar_functions.create_command_template(input_path)\n",
    "output_tif = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/corrected_tif/ASO_MRB_20180425_vbc_5.tif'\n",
    "target_assign = 'Z-0.03'#'Z-'+str(abs(ASO_20160518_hwy89_stats[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d93f1b-301e-4406-b58f-c8dbcf62cf2a",
   "metadata": {},
   "source": [
    "Create the reader stages of the pipeline. Each file is read as an individual reader stage here, and we cheat here by copying the formatting of json files and creating a dictionary with the values as the correctly formatted inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c70f3c-ffcd-4e85-b216-dcfb251fc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "writers_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fec394-70ca-404f-9fb5-67b32e934ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [730235.96, 738826.45, 4364741.66, 4372273.16, 8590.48999999999, 7531.5]\n",
    "#Awriters_bounds = '(['+ str(ASO_SCB_20160518_ext[0])+','+ str(ASO_SCB_20160518_ext[1])+'],['+str(ASO_SCB_20160518_ext[2])+','+str(ASO_SCB_20160518_ext[3])+'])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e3cdd-58ac-48a0-b748-663ea519090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all las files or stages\n",
    "filter_merge = {\"type\":\"filters.merge\",\n",
    "               \"tag\": \"merged\",\n",
    "               \"inputs\": tags}\n",
    "# filter out the ground points of the tiles\n",
    "filter_assign = {'type': 'filters.assign',\n",
    "                 'value':\"Z=\"+target_assign,\n",
    "                 'inputs':'merged',\n",
    "                 'tag':'corrected'}\n",
    "# filter out the ground points of the tiles\n",
    "#filter_range = {'type': 'filters.range',\n",
    "#                  'limits':\"Z[0.15:5]\",\n",
    "#                  'inputs':'corrected',\n",
    "#                  'tag':'ranged'}\n",
    "filter_range = {'type': 'filters.range',\n",
    "                 'limits':\"Z[:1.5]\",\n",
    "                 'inputs':'corrected',\n",
    "                 'tag':'ranged'}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'mean',\n",
    "              'resolution': '1.0',\n",
    "              'radius': '0.7',\n",
    "               'bounds': writers_bounds,\n",
    "               'inputs': 'ranged',\n",
    "               'filename':output_tif}\n",
    "# Append each stage to a list prior to saving to json \n",
    "pipeline_list = filenames.copy()\n",
    "pipeline_list.append(filter_merge)\n",
    "#pipeline_list.append(filter_assign)\n",
    "pipeline_list.append(filter_range)\n",
    "pipeline_list.append(writers_gdal)\n",
    "\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(correct_merge_rasterize_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb980f62-9d55-40a9-99cc-1136f519df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = ~3 min\n",
    "pdal_cmd = ['pdal','pipeline', correct_merge_rasterize_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810b909-6aa9-4324-a336-d1b9ac89c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lidar = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/ASO_MRB_20180425_278469_4175202.laz'\n",
    "output_tif = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/corrected_tif/ASO_MRB_20180425_278469_4175202.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29080e72-09df-4478-8097-b7dd5e80df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/corrected_tif/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.rasterize_mean, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44fcf6-4786-4635-8e55-12b8c279007e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bb43f-174d-432a-b56f-8557b110b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20180425/corrected_tif/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.rasterize_mean, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170ff33-1aac-4bba-8a3b-fd36044f62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20200413_20200414/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20200413_20200414/corrected_tif/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.rasterize_mean, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f823dc-db95-489e-a285-a4b50c9400e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/ASO/ASO_MRB_20210429/corrected_tif/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.rasterize_mean, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4789d7-2736-4059-86da-1695337b2168",
   "metadata": {},
   "source": [
    "### Rasterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b350b-b66e-4446-9570-9f40cc338c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterize_json_max = json_base_path+'rasterize_max_1pt5.json'\n",
    "# create a pipeline and save to a json file \n",
    "reader_dict = {'type':'readers.las'}\n",
    "filter_range = {'type':'filters.range',\n",
    "               'limits': 'Z[:1.5]'}\n",
    "\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "              'output_type': 'max',\n",
    "              'resolution': '1.0',\n",
    "              'radius': '0.7'}#,\n",
    "             #'window_size':3}\n",
    "\n",
    "pipeline_list = [reader_dict,filter_range,writers_gdal]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "with open(rasterize_json_max, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac35951-ba8d-48ba-98dd-540dc965e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel processing\n",
    "# time = 1.1 min\n",
    "tic = time.perf_counter()\n",
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "output_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/CHM_lt1pt5/'\n",
    "if __name__ == '__main__':\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "        full_path = [input_path + s for s in onlyfiles]\n",
    "        output_path_full = [output_path + s[:-3]+'tif' for s in onlyfiles]\n",
    "        executor.map(lidar_functions.rasterize_max, full_path, output_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af788e-b650-40c1-911e-17f91ea06b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b466837-460a-4f8b-9202-f81a226fa651",
   "metadata": {},
   "source": [
    "## Snow-Off Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa814a07-3c70-4bc8-b6d3-3b35f3336c30",
   "metadata": {},
   "source": [
    "### Vegetation Strata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cba995-877a-4a96-a828-784ec57aade0",
   "metadata": {},
   "source": [
    "**Combined Pipeline Method**\n",
    "We'll use the same bounds as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d03d8-7a0f-4f32-9f9d-6848384eca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterMergeRasterize_json = json_base_path+'filterMergeRasterize.json'\n",
    "# filterMergeRasterize_json = json_base_path+'filterMergeRasterize_neg0pt15_0pt15.json'\n",
    "# filterMergeRasterize_json = json_base_path+'filterMergeRasterize_0pt15_2.json'\n",
    "# filterMergeRasterize_json = json_base_path+'filterMergeRasterize_2.json'\n",
    "# filterMergeRasterize_json = json_base_path+'filterMergeRasterize_0pt15_0pt15.json'\n",
    "# filterMergeRasterize_json = json_base_path+'filterMergeRasterize_2_nonground.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa124ce7-908c-4981-947b-57d296be799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "output_tif = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/veg_strata/veg_classes/vegStrata_3.tif'\n",
    "filenames, tags = lidar_functions.create_command_template(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f04675-a114-4c79-b9a6-8bd0ec645a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all las files or stages\n",
    "filter_merge = {\"type\":\"filters.merge\",\n",
    "               \"tag\": \"merged\",\n",
    "               \"inputs\": tags}\n",
    "# filter out the ground points of the tiles\n",
    "filter_range = {\"type\":\"filters.range\",\n",
    "                \"limits\":\"Z[3:)\",\n",
    "               'inputs':'merged',\n",
    "               'tag':'filtered'}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'count',\n",
    "              'resolution': '1.0',\n",
    "              'radius': '0.7',\n",
    "               'bounds': writers_bounds,\n",
    "               'inputs': 'filtered',\n",
    "               'filename':output_tif}\n",
    "# Append each stage to a list prior to saving to json \n",
    "pipeline_list = filenames.copy()\n",
    "pipeline_list.append(filter_merge)\n",
    "pipeline_list.append(filter_range)\n",
    "pipeline_list.append(writers_gdal)\n",
    "\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(filterMergeRasterize_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850dd26e-cc34-4731-9c2a-1b27b0d9c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b3ba5-0b1c-4f7c-a068-180d263aba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdal_cmd = ['pdal','pipeline', filterMergeRasterize_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464b547-4114-43ad-8c6c-7ac3256b62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4c3fe-af8d-4b65-b53f-04338df3d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(toc-tic) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f8358-6e91-4889-b3bc-e1300bea86f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eee961-41c0-47e1-b6b3-f7a492816548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a0bf40-98c0-423a-8b6a-a1a587782508",
   "metadata": {},
   "source": [
    "### Create CHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e3961-714c-49fd-bbd5-022c07a0b8d6",
   "metadata": {},
   "source": [
    "Note that there are two CHMs for these data. One is based off all points, another is limited to possible short/understory vegetation (<1.5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3107c9e-e6db-46b8-96f6-d10b3a5d49ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHM_json_LT1pt5 = json_base_path+'CHM_from_las_LT1pt5.json' # define path to json files\n",
    "CHM_json = json_base_path+'CHM_from_las.json' # define path to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b1a911e-a30c-47d0-b961-4d5b8818a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/HAG/'\n",
    "output_tif = 'MRB/Merced_lidar/NCALM/NCALM_MRB_20180921/CHM/NCALM_MRB_20180921_CHM_2m.tif'\n",
    "onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n",
    "input_list = [input_path + s for s in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae874a5e-2129-4105-98da-fae42d42026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dict = {}\n",
    "tags = ['']*len(input_list)\n",
    "filenames = ['']*len(input_list)\n",
    "for i in range(len(input_list)):\n",
    "    filename_dict['filename_'+str(i)] = {'filename':input_list[i], 'tag':'A_'+str(i)}\n",
    "    tags[i] = 'A_'+str(i)\n",
    "    filenames[i] = filename_dict[list(filename_dict)[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93c5a436-3444-4b72-8fd0-bb837b12087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all las files or stages\n",
    "filter_merge = {\"type\":\"filters.merge\",\n",
    "               \"tag\": \"merged\",\n",
    "               \"inputs\": tags}\n",
    "# filter_return = {'type': 'filters.returns',\n",
    "#                 'groups': 'first',\n",
    "#                 'inputs':'merged',\n",
    "#                 'tag':'filtered'}\n",
    "filter_range = {'type': 'filters.range',\n",
    "                'limits': 'Z[2:100)',\n",
    "                'inputs':'merged',\n",
    "                'tag':'ranged'}\n",
    "# write merged las to raster\n",
    "writers_gdal= {\"type\": \"writers.gdal\",\n",
    "               'output_type': 'max', \n",
    "              'resolution': '1.0',\n",
    "               'bounds': writers_bounds,\n",
    "              'radius': '0.7',\n",
    "               'filename': output_tif}\n",
    "# Append each stage to a list prior to saving to json \n",
    "pipeline_list = filenames.copy()\n",
    "pipeline_list.append(filter_merge)\n",
    "#pipeline_list.append(filter_range)\n",
    "pipeline_list.append(writers_gdal)\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# save to json\n",
    "with open(CHM_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "762b779c-8d65-4b9d-ab1b-240eb55fcbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c666f24-f4ec-419d-b056-b8e3c3557ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pdal', 'pipeline', 'piske_processing/PDAL_workflow/JSON/CHM_from_las.json'], returncode=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdal_cmd = ['pdal','pipeline', CHM_json]\n",
    "subprocess.run(pdal_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "872d681c-e0ab-4278-863d-04927bb403e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5fa43794-5cd5-4afd-8a32-a06882b7d96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.259989133333388"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(toc-tic)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef33d89-f517-4fef-b0bc-f8ff3ad9b571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe8463e-2c0e-4d0d-b2a7-1b132a8e1f4e",
   "metadata": {},
   "source": [
    "**Move to Raster workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a658118-ad67-41a7-97e2-7d64913b8d80",
   "metadata": {},
   "source": [
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221a885-025a-44c5-a762-a9fdafc5a11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a4c5e-29e3-4dce-93b0-3639e61925d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c740714-5881-4763-bcb2-b2387741f477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd18c8-82e0-4a86-b878-9a53710554f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26833f00-aa67-426b-8e20-c1c3eff46862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56715656-b594-4285-9b2a-5a875aaabd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33e66bb1-b42b-445d-af25-de4b376c9652",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf738b4-4eae-41dc-9cc8-898b177ac071",
   "metadata": {},
   "source": [
    "## File format conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10eab7-1dc5-4561-9ff8-dbabb55abf57",
   "metadata": {},
   "source": [
    "### Convert .las to .txt\n",
    "see: https://pdal.io/stages/writers.text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c745d-b984-41bc-a04a-b54dcfbbaf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069497d0-a99b-4952-bfa2-3af3d3c0ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up json file commands\n",
    "output_txt = '/Volumes/cpiske/lidar_processing/python_scripts/PDAL/test_las/mcc_part_b_tile_004_000Test.asc'\n",
    "output_json = 'lidar_processing/python_scripts/PDAL_workflow/JSON/las_to_txt.json'\n",
    "\n",
    "# create a pipeline and save to a json file \n",
    "\n",
    "filter_dict = {'type':'readers.las',\n",
    "               'override_srs': \"EPSG:4326\",\n",
    "              'filename': input_las} # we are reading in a las file\n",
    "rasterize_dict = {'type':'writers.las',\n",
    "'format':'geojson',\n",
    "'order':'X,Y,Z',\n",
    "'keep_unspecified':'false',\n",
    "'filename':output_txt}\n",
    "\n",
    "\n",
    "pipeline_list = [filter_dict, rasterize_dict]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "with open(output_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f570972-2f18-40d1-b4d2-06f0ed5ec7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'lidar_processing/python_scripts/PDAL/JSON/las_to_txt.json'\n",
    "pdal_commands = ['pdal', 'pipeline', json_path]\n",
    "subprocess.run(pdal_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a3a30-1ae6-481b-96ac-f95a1f6f4ddc",
   "metadata": {},
   "source": [
    "### .laz to .las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe33f9e-84de-49ea-a538-0cd37b7f7877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # set up json file commands\n",
    "# input_laz = 'lidar_processing/python_scripts/PDAL/test_las/ASO_USCAMB20180425f1a1_180425_1_dem_filter.laz'\n",
    "# output_las = 'lidar_processing/python_scripts/PDAL/test_las/ASO_USCAMB20180425f1a1_180425_1_dem_filter.las'\n",
    "# output_json = 'lidar_processing/python_scripts/PDAL/JSON/laz_to_las.json'\n",
    "\n",
    "# # create a pipeline and save to a json file \n",
    "\n",
    "# filter_dict = {'type':'readers.las',\n",
    "#                'filename': input_las} # we are reading in a las file\n",
    "# translate_dict = {'type':'writers.las',\n",
    "#                   \"a_srs\": \"EPSG:4326\",\n",
    "#                   'filename':output_las}\n",
    "\n",
    "\n",
    "# pipeline_list = [filter_dict, translate_dict]\n",
    "# pipeline_dict = {'pipeline' : pipeline_list}\n",
    "# with open(output_json, 'w') as out:\n",
    "#     json.dump(pipeline_dict, out, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb37c88-c5d9-4859-91ab-5c35e53bd514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe40151-4f3c-4fc8-acc0-ff2e092d788b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# json_path = 'lidar_processing/python_scripts/PDAL/JSON/laz_to_las.json'\n",
    "# pdal_commands = ['pdal', 'pipeline', json_path]\n",
    "# subprocess.run(pdal_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea292cbb-156e-4be0-9fa8-62560bd55feb",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Raster Caluclations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b90302-4ff2-461e-84b4-28f68e90575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2,3,4,5]:\n",
    "    apr_elev = 'lidar_processing/PDAL_testFiles_tutorials/test_las/SCB/rasterize/ASO_20160417/mcc_part_b_tile_004_00'+str(i)+'.tif'\n",
    "    may_elev = 'lidar_processing/PDAL_testFiles_tutorials/test_las/SCB/rasterize/ASO_20160518/mcc_part_b_tile_004_00'+str(i)+'.tif'\n",
    "    output = 'lidar_processing/PDAL_testFiles_tutorials/test_las/SCB/raster_subtract/mcc_part_b_tile_004_00'+str(i)+'.tif'\n",
    "    raster_sub = ['gdal_calc.py', '-a', apr_elev, '-b', may_elev, '--calc=\"a - b\"', '--outfile', output]\n",
    "    subprocess.run(raster_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95aabd-bd70-47a0-be17-993b89088961",
   "metadata": {},
   "source": [
    "# Theo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a85fc-2834-4d0d-81a9-19f790b08678",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_las = 'SCB/kost_lidar_data/ASO_2016/2016_05_18/WGS84_G1762_to_NAD83_NAVD88/mcc_part_b_tile_004_000.las'\n",
    "\n",
    "z_min = 0.15\n",
    "z_max = 2\n",
    "z_range = 'Z[' + str(z_min) + ':' + str(z_max) + ']'\n",
    "output_raster = 'lidar_processing/python_scripts/PDAL/test_file/mcc_part_b_tile_004_000.tif'\n",
    "output_json = 'lidar_processing/python_scripts/PDAL/JSON/las_to_tif.json'\n",
    "resolution = 0.01\n",
    "\n",
    "filter_dict = {'type':'filters.range', 'limits':z_range}\n",
    "rasterize_dict = {'filename':output_raster,\n",
    "'gdaldriver':'GTiff',\n",
    "'output_type':'count',\n",
    "'resolution':resolution,\n",
    "'type': 'writers.gdal'}\n",
    "\n",
    "\n",
    "pipeline_list = [input_las, filter_dict, rasterize_dict]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "with open(output_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929f364-00c7-482e-8d5f-537652eed3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_las = 'SCB/kost_lidar_data/ASO_2016/2016_05_18/WGS84_G1762_to_NAD83_NAVD88/mcc_part_b_tile_004_000.las'\n",
    "\n",
    "# z_min = 0.15\n",
    "# z_max = 2\n",
    "# z_range = 'Z[' + str(z_min) + ':' + str(z_max) + ']'\n",
    "output_raster = 'lidar_processing/python_scripts/PDAL/test_file/mcc_part_b_tile_004_000.asc'\n",
    "output_json = 'lidar_processing/python_scripts/PDAL/JSON/las_to_asc.json'\n",
    "resolution = 0.01\n",
    "\n",
    "filter_dict = {'type':'filters.range', 'limits':z_range}\n",
    "rasterize_dict = {'filename':output_raster,\n",
    "'gdaldriver':'XYZ',\n",
    "'output_type':'count',\n",
    "'resolution':resolution,\n",
    "'type': 'writers.gdal'}\n",
    "\n",
    "\n",
    "pipeline_list = [input_las, filter_dict, rasterize_dict]\n",
    "pipeline_dict = {'pipeline' : pipeline_list}\n",
    "with open(output_json, 'w') as out:\n",
    "    json.dump(pipeline_dict, out, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f6338-edab-4c85-8941-b05d63584dd8",
   "metadata": {},
   "source": [
    "sudo docker run -v path_to_laz_folder:/input 0b pdal info /input/test.laz\n",
    "\n",
    "where path_to_laz_folder is the path to the LAS/LAZ file (you just need the folder path, not the file path).\n",
    "\n",
    ":/input is the new folder that will be created in your Docker container that will hold your point cloud.\n",
    "\n",
    "0b is just the image id of pdal\n",
    "\n",
    "/input/test.laz is the path to the point cloud in the Docker container.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941783e-828f-4226-a25a-b772a51d12d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_laz_folder = 'lidar_processing/python_scripts/PDAL/test_las'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee8f9b-8d85-490a-a3cd-22d4b06bb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker run -v path_to_laz_folder:/input 0b pdal info /input/test.laz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac352849-3336-4cd7-b7d4-2f0c2a2c79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import pdal\n",
    "\n",
    "def assemblePipeline(input_las, list_of_dicts):\n",
    "    pipeline_list = [input_las]\n",
    "    pipeline_list.extend(list_of_dicts)\n",
    "    pipeline_dict = {'pipeline' : pipeline_list}\n",
    "    return pipeline_dict\n",
    "\n",
    "def makeHeightFilter(height, buffer):\n",
    "    z_min = height - buffer/2\n",
    "    z_max = height + buffer/2\n",
    "    z_range = 'Z[' + str(z_min) + ':' + str(z_max) + ']'\n",
    "    heightDict = {'type':'filters.range', 'limits':z_range}\n",
    "    return heightDict\n",
    "\n",
    "def makeRasterizeFilter(output_raster, resolution, epsg):\n",
    "    rasterize_dict = {'filename':output_raster,\n",
    "                      'gdaldriver':'GTiff',\n",
    "                      'output_type':'count',\n",
    "                      'resolution':resolution,\n",
    "                      'override_srs' : epsg,\n",
    "                      'type': 'writers.gdal'}\n",
    "    return rasterize_dict\n",
    "\n",
    "def convertTifForPIL(input_raster, output_raster, epsg):\n",
    "    ''' GDAL bindings are an alien concept to me, so I gave up and used\n",
    "        subprocess.'''\n",
    "    commands = ['gdal_translate', input_raster, output_raster, '-ot', 'Byte', '-a_srs', epsg]\n",
    "    subprocess.run(commands)\n",
    "\n",
    "\n",
    "def buildHeightSlice(input_las, height, buffer, output_raster, resolution, epsg, json_path=None):\n",
    "    filter_dict = makeHeightFilter(height, buffer)\n",
    "    rasterize_dict = makeRasterizeFilter(output_raster, resolution, epsg)\n",
    "    filter_list = [filter_dict, rasterize_dict]\n",
    "    pipeline_dict = assemblePipeline(input_las, filter_list)\n",
    "\n",
    "    if json_path is not None:\n",
    "        with open(json_path, 'w') as out:\n",
    "            json.dump(pipeline_dict, out, indent=4)\n",
    "        pdal_commands = ['pdal', 'pipeline', json_path]\n",
    "        subprocess.run(pdal_commands)\n",
    "    else:\n",
    "        pdal_commands = json.dumps(pipeline_dict)\n",
    "        pipeline = pdal.Pipeline(pdal_commands)\n",
    "        pipeline.execute()\n",
    "\n",
    "input_las = '/Users/theo/data/las/TLS_0244_20180612_01_v003_30m_clip_height_norm.las'\n",
    "height = 1.37\n",
    "buffer = 0.05\n",
    "z_min = height - buffer/2\n",
    "z_max = height + buffer/2\n",
    "z_range = 'Z[' + str(z_min) + ':' + str(z_max) + ']'\n",
    "temp_raster = '/Users/theo/Pictures/almost_cool.tif'\n",
    "final_raster = '/Users/theo/Pictures/cool.tif'\n",
    "resolution = 0.01\n",
    "epsg = 'EPSG:3310'\n",
    "\n",
    "buildHeightSlice(input_las, height, buffer, temp_raster, resolution, epsg)\n",
    "convertTifForPIL(temp_raster, final_raster, epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d188910-a9a7-4c6c-b164-8a163c1d5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import pdal\n",
    "import argparse\n",
    "\n",
    "# Create flags for the user to utilize.\n",
    "parser = argparse.ArgumentParser(description=\"Generate JSON pipeline to generate DTM from a point cloud.\")\n",
    "      \n",
    "required = parser.add_argument_group('Required arguments')\n",
    "required.add_argument('-crs', '--coordinate_system', required=True, action='store', help=\"EPSG code.\")\n",
    "required.add_argument('-i', '--infile', required=True, action='store', help=\"Input path to point cloud\")\n",
    "required.add_argument('-o', '--outfile', required=True, action='store', help=\"Output path.\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "def generateJSON(infile, list_of_dicts):\n",
    "    pipeline_list = [infile]\n",
    "    pipeline_list.extend(list_of_dicts)\n",
    "    pipeline_dict = {'pipeline': pipeline_list}\n",
    "    with open(\"pipeline.json\", 'w') as out:\n",
    "        json.dump(pipeline_dict, out, indent=4)\n",
    "\n",
    "def generateDTM(epsg, infile, outfile):\n",
    "    reproject_dict = {\"type\": \"filters.reprojection\",\n",
    "                      \"out_srs\": \"EPSG:{}\".format(epsg)}\n",
    "    reclassify_zero_dict = {\"type\": \"filters.assign\",\n",
    "                       \"assignment\": \"Classification[:]=0\"}\n",
    "    elm_dict = {\"type\": \"filters.elm\"}\n",
    "    outlier_dict = {\"type\": \"filters.outlier\"}\n",
    "    smrf_dict = {\"type\": \"filters.smrf\", \"ignore\": \"Classification[7:7]\",\n",
    "                 \"slope\": 0.2, \"window\": 16, \"threshold\": 0.45, \"scalar\": 1.2}\n",
    "    range_dict = {\"type\":\"filters.range\", \"limits\":\"Classification[2:2]\"}\n",
    "    output_dict = {\"filename\": outfile, \"gdaldriver\": \"GTiff\", \"output_type\": \"all\", \"resolution\": 0.01, \"type\": \"writers.gdal\"}\n",
    "    list_of_dicts = list([reproject_dict, reclassify_zero_dict, elm_dict, outlier_dict, smrf_dict, range_dict, output_dict])\n",
    "    generateJSON(infile, list_of_dicts)\n",
    "    pdal_cmds = ['pdal', 'pipeline', 'pipeline.json']\n",
    "    subprocess.run(pdal_cmds)\n",
    "    \n",
    "generateDTM(args.coordinate_system, args.infile, args.outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efcfb16-ecfc-4ce2-8a5e-6bf2974c4510",
   "metadata": {},
   "source": [
    "# More helpful things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84929bbc-a6c6-49ad-bfc5-9789fe5dd577",
   "metadata": {},
   "source": [
    "### Get stats of a dataset\n",
    "see: https://www.spatialised.net/lidar-qa-with-pdal-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7713f5-fbfd-412c-8488-04489fa868db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name JSON file\n",
    "\n",
    "stats_json = 'lidar_processing/python_scripts/PDAL_workflow/JSON/stats.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3f4a2-adf9-4d2e-a62e-a2a538437396",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_las = {\"type\":\"readers.las\",\n",
    "              \"filename\": input_las_stats}\n",
    "filter_stats = {\"type\":\"filters.stats\",\n",
    "                \"dimensions\":\"Z\",\n",
    "                \"global\":\"Z\",\n",
    "                \"advanced\":\"true\"}\n",
    "pipeline_list = [reader_las, filter_stats]\n",
    "#pipeline_dict = {reader_las, filter_stats}\n",
    "# with open(stats_json, 'w') as out:\n",
    "#     json.dump(pipeline_dict, out, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5cfca-34a5-471d-8b40-bef477ef6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a58e53-92c7-47c2-b75b-f9c9466b9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pdal.Pipeline(json.dumps(pipeline_list))\n",
    "pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c7605-47f2-4698-bec0-f2aa20a26f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(pipeline.metadata)[\"metadata\"][\"filters.stats\"][\"statistic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67b19a-0d9f-4b10-ad08-ccf9593fa659",
   "metadata": {},
   "source": [
    "## Navigating folders/files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67984138-2046-46f9-9187-26121ac09dca",
   "metadata": {},
   "source": [
    "**create list of files/folders with a wildcard (*)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b23dd1-01e1-46d4-81e7-b9e5be2df38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex. list all files in folder4 that end in .laz = folder1/folder2/folder3/folder4/*.laz\n",
    "# ex. list all folders named folder3 in folder 1 = foler1/*/folder3\n",
    "# ex. list all list all contents in folder2 = folder2/* - note just folder 2, no subdirectories\n",
    "glob_cmd = 'path'\n",
    "glob_exe = glob.glob(file_glob_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124af7f3-340e-4a3d-8b74-fbff724635f2",
   "metadata": {},
   "source": [
    "**create a list with all directories/subdirectories on a path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d4080-2f26-48b8-b908-992955d0200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = [x[0] for x in os.walk('path')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddee80f-1c7f-46e1-a8e8-2cee82ac6f02",
   "metadata": {},
   "source": [
    "**create a list with all directories/subdirectories on a path with specific folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7ef3c-3534-4566-9a4d-63ce9bcd074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = [x[0] for x in os.walk('path')]\n",
    "index_pos_list = [ i for i in range(len(all_folders)) if all_folders[i][-6:] == 'retile' ] # must change this to meet requirements\n",
    "full_list = [all_folders[i] for i in index_pos_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fb6c4-a80d-4b97-8533-1852b3a8e992",
   "metadata": {},
   "source": [
    "**get name of the directory just above one listed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cc0c9-4dcb-44e7-a72f-8586c6828636",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirname = os.path.basename(os.path.dirname('path'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141720ac-a112-4891-b727-3e4999ed4298",
   "metadata": {},
   "source": [
    "**create a list with only filenames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412acaf-9f08-430c-8ce2-ca0027d4d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = [f for f in os.listdir('path') if os.path.isfile(os.path.join('path', f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80f4b2-ebb0-4eb2-9a39-58d554be97b0",
   "metadata": {},
   "source": [
    "**create a list with full file paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc96133-4087-4aca-a90b-302487a803e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = ['path' + '/' + s for s in onlyfiles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a6064-f348-405d-bf7d-5e8c8f7bfd81",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215beb4e-46bb-4271-b452-6849510f3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = 142.5\n",
    "tic = time.perf_counter()\n",
    "list(map(function, args));\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ca6cc-9fb5-48ce-8146-b41150dd9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = 53 s\n",
    "tic = time.perf_counter()\n",
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(3)\n",
    "    pool.map(function, arg)\n",
    "    pool.close()\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e3ba6-7d5f-4a00-b563-ee2a53074b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = very fast? .06s\n",
    "tic = time.perf_counter()\n",
    "if __name__ == \"__main__\":\n",
    "    executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)\n",
    "    executor.map(function, arg)\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c6986-5a20-472a-a5bd-4a854161ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time = 0.22\n",
    "tic = time.perf_counter()\n",
    "if __name__ == \"__main__\":\n",
    "    executor = concurrent.futures.ProcessPoolExecutor(max_workers=3)\n",
    "    executor.map(function, arg)\n",
    "toc = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f97306-b0c1-4b9d-8b1e-5bd95d1732c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
